{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -U numpy pandas matplotlib scikit-learn tensorflow[and-cuda] tensorflow-datasets kaggle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Num GPUs Available: \", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kaggle\n",
    "# from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "# import os\n",
    "# import zipfile\n",
    "\n",
    "# api = KaggleApi()\n",
    "# api.authenticate()\n",
    "\n",
    "# dataset_path = \"ham10000_data\"\n",
    "\n",
    "# if not os.path.exists(dataset_path):\n",
    "#     os.makedirs(dataset_path)\n",
    "\n",
    "# api.dataset_download_files('kmader/skin-cancer-mnist-ham10000', path=dataset_path, unzip=True)\n",
    "\n",
    "# zip_file_path = os.path.join(dataset_path, 'skin-cancer-mnist-ham10000.zip')\n",
    "# if os.path.exists(zip_file_path):\n",
    "#     with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "#         zip_ref.extractall(dataset_path)\n",
    "#     print(f\"Dataset extracted to: {dataset_path}\")\n",
    "# else:\n",
    "#     print(\"Dataset already extracted or no zip file found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls ham10000_data/images/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the CSV file\n",
    "csv_path = 'ham10000_data/HAM10000_metadata.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "image_dir = 'ham10000_data/images/'\n",
    "\n",
    "# Create a mapping of labels to integers (for one-hot encoding)\n",
    "label_mapping = {label: idx for idx, label in enumerate(df['dx'].unique())}\n",
    "df['label'] = df['dx'].map(label_mapping)\n",
    "\n",
    "# Print the first few rows to verify\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_id, label):\n",
    "    # Use TensorFlow's tf.strings.join instead of os.path.join\n",
    "    image_path = tf.strings.join([image_dir, image_id + \".jpg\"], separator=\"/\")\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, (224, 224))  # ResNet expects 224x224 images\n",
    "    image = tf.keras.applications.resnet50.preprocess_input(image)  # Preprocessing for ResNet\n",
    "\n",
    "    return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame columns to TensorFlow tensors\n",
    "image_ids = df['image_id'].values\n",
    "labels = df['label'].values\n",
    "\n",
    "# Create a TensorFlow dataset from the image IDs and labels\n",
    "dataset = tf.data.Dataset.from_tensor_slices((image_ids, labels))\n",
    "\n",
    "# Map the load_image function to each element in the dataset\n",
    "dataset = dataset.map(lambda image_id, label: load_image(image_id, label))\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Shuffle the entire dataset before splitting\n",
    "dataset = dataset.shuffle(buffer_size=len(df))\n",
    "\n",
    "# Split into training and validation datasets (80/20 split)\n",
    "train_size = int(0.8 * len(df))\n",
    "train_dataset = dataset.take(train_size)\n",
    "val_dataset = dataset.skip(train_size)\n",
    "\n",
    "# Apply batching, repeating, and prefetching to the training dataset\n",
    "train_dataset = train_dataset.batch(batch_size).repeat().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Apply batching, repeating, and prefetching to the validation dataset\n",
    "val_dataset = val_dataset.batch(batch_size).repeat().prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet152\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope():\n",
    "    # Load the ResNet50 model with pre-trained weights\n",
    "    resnet_model = ResNet152(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "    # Add custom classification layers\n",
    "    x = GlobalAveragePooling2D()(resnet_model.output)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dense(7, activation='softmax')(x)  # 7 classes in HAM10000\n",
    "\n",
    "    # Create the final model\n",
    "    model = Model(inputs=resnet_model.input, outputs=x)\n",
    "\n",
    "    # Freeze the base ResNet50 layers\n",
    "    for layer in resnet_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "steps_per_epoch = train_size // batch_size\n",
    "validation_steps = (len(df) - train_size) // batch_size\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "train_dataset = train_dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.cache()\n",
    "\n",
    "val_dataset = val_dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential([\n",
    "    keras.layers.RandomFlip('horizontal'),\n",
    "    keras.layers.RandomRotation(0.1),\n",
    "    keras.layers.RandomZoom(0.1),\n",
    "    keras.layers.RandomContrast(0.1),\n",
    "    keras.layers.RandomTranslation(0.1, 0.1),\n",
    "])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
