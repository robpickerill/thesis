{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade tensorflow keras scikit-learn opencv-python matplotlib keras_tuner keras_cv numpy seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow INFO and WARNING messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import pandas as pd\n",
    "import os\n",
    "import keras_cv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'ham10000_data/HAM10000_metadata.csv'\n",
    "img_dir = 'ham10000_data/images'\n",
    "file_ext = '.jpg'\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "image_paths = [\n",
    "    os.path.join(img_dir, image_id + file_ext)\n",
    "    for image_id in df['image_id']\n",
    "]\n",
    "\n",
    "print(f\"Number of images: {len(image_paths)}\")\n",
    "print(f\"First 5 image paths: {image_paths[:5]}\")\n",
    "\n",
    "classes = sorted(df['dx'].unique())\n",
    "num_classes = len(classes)\n",
    "\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Classes: {classes}\")\n",
    "\n",
    "labels = df['dx'].map(lambda x: classes.index(x)).values\n",
    "class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "\n",
    "print(f\"Class to Index Mapping: {class_to_idx}\")\n",
    "print(f\"Labels: {labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "value_counts = df['dx'].value_counts()\n",
    "\n",
    "plt.bar(value_counts.index, value_counts.values)\n",
    "plt.title('Distribution of Lesion Categories')\n",
    "plt.xlabel('Lesion Category')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import random\n",
    "# import tensorflow as tf\n",
    "\n",
    "# n_samples_per_class = 5\n",
    "\n",
    "# plt.figure(figsize=(15, len(classes) * 3))\n",
    "\n",
    "# for class_index, class_name in enumerate(classes):\n",
    "#     class_indices = [i for i, label in enumerate(labels) if label == class_index]\n",
    "\n",
    "#     random_class_indices = random.sample(class_indices, min(n_samples_per_class, len(class_indices)))\n",
    "\n",
    "#     for i, idx in enumerate(random_class_indices):\n",
    "#         img_path = image_paths[idx]\n",
    "#         label = labels[idx]\n",
    "\n",
    "#         img = tf.io.read_file(img_path)\n",
    "#         img = tf.image.decode_jpeg(img, channels=3)\n",
    "\n",
    "#         # Display image\n",
    "#         plt.subplot(len(classes), n_samples_per_class, class_index * n_samples_per_class + i + 1)\n",
    "#         plt.imshow(img)\n",
    "#         plt.title(f\"Class: {class_name}\")\n",
    "#         plt.axis('off')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_id, label):\n",
    "    # Load and preprocess the image\n",
    "    image = tf.io.read_file(image_id)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, (224, 224))  # ResNet expects 224x224 images\n",
    "    image = tf.keras.applications.resnet50.preprocess_input(image)  # Preprocessing for ResNet\n",
    "\n",
    "    return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "train_paths = []\n",
    "train_labels = []\n",
    "val_paths = []\n",
    "val_labels = []\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, val_index in sss.split(image_paths, labels):\n",
    "    train_paths = [image_paths[i] for i in train_index]\n",
    "    train_labels = labels[train_index]\n",
    "    val_paths = [image_paths[i] for i in val_index]\n",
    "    val_labels = labels[val_index]\n",
    "\n",
    "print(f\"Training set size: {len(train_paths)}\")\n",
    "print(f\"Validation set size: {len(val_paths)}\")\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "train_dataset = train_dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.cache()\n",
    "\n",
    "val_dataset = val_dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential([\n",
    "    keras.layers.RandomFlip('horizontal'),\n",
    "    keras.layers.RandomRotation(0.1),\n",
    "    keras.layers.RandomZoom(0.1),\n",
    "    keras.layers.RandomContrast(0.1),\n",
    "    keras.layers.RandomTranslation(0.1, 0.1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class_counts = np.bincount(train_labels)\n",
    "total_counts = np.sum(class_counts)\n",
    "class_prior = class_counts / total_counts\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "    with strategy.scope():\n",
    "        base_model = keras.applications.resnet.ResNet101(\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            input_shape=(224, 224, 3),\n",
    "            pooling=None\n",
    "        )\n",
    "\n",
    "        base_model.trainable = False\n",
    "\n",
    "        inputs = keras.Input(shape=(224, 224, 3))\n",
    "        # x = data_augmentation(inputs)\n",
    "        x = base_model(inputs, training=False)\n",
    "        x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "        # # Tune Dropout Rate\n",
    "        # dropout_rate = hp.Float('dropout_rate', min_value=0.0, max_value=0.7, step=0.1)\n",
    "        # if dropout_rate > 0.0:\n",
    "        #     x = keras.layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "        output_bias = np.log(class_prior)\n",
    "\n",
    "        # Output Layer\n",
    "        outputs = keras.layers.Dense(\n",
    "            num_classes,\n",
    "            activation='softmax',\n",
    "            bias_initializer=keras.initializers.Constant(output_bias),\n",
    "            # kernel_regularizer=keras.regularizers.l2(\n",
    "            #     hp.Float('l2_regularization', min_value=0.0, max_value=0.1, step=0.01)\n",
    "            # )\n",
    "        )(x)\n",
    "\n",
    "        model = keras.Model(inputs, outputs)\n",
    "\n",
    "        learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5, 1e-6])\n",
    "\n",
    "        optimizer = keras.optimizers.AdamW(\n",
    "            learning_rate=learning_rate,\n",
    "            weight_decay=hp.Float('weight_decay', min_value=0.0, max_value=0.01, step=0.001)\n",
    "        )\n",
    "\n",
    "        # Conditionally select loss function\n",
    "        # loss_choice = hp.Choice('loss_function', values=['sparse_categorical_crossentropy', 'focal_loss'])\n",
    "\n",
    "        # if loss_choice == 'focal_loss':\n",
    "        #     loss = CategoricalFocalLoss(alpha=0.25, gamma=2)\n",
    "        #     class_weights = None\n",
    "        # else:\n",
    "        #     loss = 'sparse_categorical_crossentropy'\n",
    "        #\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner import RandomSearch\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=20,\n",
    "    executions_per_trial=1,\n",
    "    directory='random_search',\n",
    "    project_name='hyperparameter_tuning-resnet101'\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "Hyperparameters:\n",
    "Best Learning Rate: {best_hps.get('learning_rate')}\n",
    "Best Weight Decay: {best_hps.get('weight_decay')}\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        min_delta=1e-4,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.1,\n",
    "        patience=5,\n",
    "        min_lr=1e-7\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath='models/resnet50-ham10000.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        verbose=1\n",
    "    ),\n",
    "]\n",
    "\n",
    "history = best_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=500,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "best_model.save('models/resnet50-ham10000-final.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "def get_predictions(dataset):\n",
    "    for images, labels in dataset:\n",
    "        predictions = best_model.predict(images)\n",
    "        y_true.extend(labels.numpy())\n",
    "        y_pred.extend(np.argmax(predictions, axis=1))\n",
    "    return np.array(y_true), np.array(y_pred)\n",
    "\n",
    "# Get predictions for validation dataset\n",
    "y_true, y_pred = get_predictions(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=classes, yticklabels=classes)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class Accuracy Plot\n",
    "class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=classes, y=class_accuracies)\n",
    "plt.title('Per-class Accuracy')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution in training set\n",
    "train_labels = np.concatenate([labels.numpy() for _, labels in train_dataset])\n",
    "train_class_dist = np.bincount(train_labels) / len(train_labels)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=classes, y=train_class_dist)\n",
    "plt.title('Class Distribution in Training Set')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Proportion')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
